# Learning Note for "How to Win a Data Science Competition: Learn from Top Kagglers" course

This repository contains programming assignments notebooks for the [course](https://www.coursera.org/learn/competitive-data-science/home/welcome) about competitive data science.

## Week 1

* Introduction & Recap of main ML algorithms
* Feature Preprocessing and Generation with Respect to Models
* Final Project Description

[Week 1 note](./Notes/Week1.md)


## Week 2

* Exploratory data analysis
* Validation
* Data leakages

[Week 2 note](./Notes/Week2.md)


## Week 3

* Metrics optimization
* Mean encodings

[Week 3 note](./Notes/Week3.md)


## Week 4

### Hyperparameter tuning

* [Tuning the hyper-parameters of an estimator (sklearn)](http://scikit-learn.org/stable/modules/grid_search.html)
* [Optimizing hyperparameters with hyperopt](http://fastml.com/optimizing-hyperparams-with-hyperopt/)
* [Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)

### Tips and tricks

* [Far0n's framework for Kaggle competitions "kaggletils"](https://github.com/Far0n/kaggletils)
* [28 Jupyter Notebook tips, tricks and shortcuts](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)

### Advanced features II

#### Matrix Factorization:
* [Overview of Matrix Decomposition methods (sklearn)](http://scikit-learn.org/stable/modules/decomposition.html)

#### t-SNE:
* [Multicore t-SNE implementation](https://github.com/DmitryUlyanov/Multicore-TSNE)
* [Comparison of Manifold Learning methods (sklearn)](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html)
* [How to Use t-SNE Effectively (distill.pub blog)](https://distill.pub/2016/misread-tsne/)
* [tSNE homepage (Laurens van der Maaten)](https://lvdmaaten.github.io/tsne/)
* [Example: tSNE with different perplexities (sklearn)](http://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py)

#### Interactions:
* [Facebook Research's paper about extracting categorical features from trees](https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/)
* [Example: Feature transformations with ensembles of trees (sklearn)](http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)

### Ensembling

* [Kaggle ensembling guide at MLWave.com (overview of approaches)](https://mlwave.com/kaggle-ensembling-guide/)
* [StackNet °X a computational, scalable and analytical meta modelling framework (by KazAnova)](https://github.com/kaz-Anova/StackNet)
* [Heamy °X a set of useful tools for competitive data science (including ensembling)](https://github.com/rushter/heamy)


## Week 5

### Competitions go through

You can often find a solution of the competition you're interested on its forum. Here we put links to collections of such solutions that will prove useful to you.

[Final project Note](Final_Project.md)


## All Materials

[Materials for course](./Notes/Materials.md)

<br/>

#### Past solutions
* http://ndres.me/kaggle-past-solutions/
* https://www.kaggle.com/wiki/PastSolutions
* http://www.chioka.in/kaggle-competition-solutions/
* https://github.com/ShuaiW/kaggle-classification/

